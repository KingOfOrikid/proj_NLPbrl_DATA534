{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e8e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"X-RosetteAPI-Key\": \"ad9765417caa9e2fd304e0aea0e2d24f\",\n",
    "            \"X-RapidAPI-Key\":\n",
    "            \"bc44666f27msh0cea06e275c5146p185e39jsnb306b7888971\",\n",
    "            \"X-RapidAPI-Host\": \"rosetteapi-rosette-v1.p.rapidapi.com\"\n",
    "        }\n",
    "        self.payload = {}\n",
    "\n",
    "    def get_token(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://rosetteapi-rosette-v1.p.rapidapi.com/tokens\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['tokens']\n",
    "\n",
    "    def get_lang(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/language\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)[\"languageDetections\"][0]['language']\n",
    "\n",
    "    def get_vec(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/semantics/vector\"\n",
    "        self.payload['content'] = text\n",
    "        self.payload['options'] = {\"perToken\": True}\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return {\n",
    "            'documentEmbedding': eval(response.text)['documentEmbedding'],\n",
    "            'tokenEmbeddings': eval(response.text)['tokenEmbeddings']\n",
    "        }\n",
    "\n",
    "    def get_posTag(self, text):\n",
    "        self.payload = {}\n",
    "        null = ''\n",
    "        url = \"https://rosetteapi-rosette-v1.p.rapidapi.com/morphology/complete\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "\n",
    "        return {\n",
    "            \"tokens\": eval(response.text)[\"tokens\"],\n",
    "            \"posTags\": eval(response.text)[\"posTags\"]\n",
    "        }\n",
    "\n",
    "    def get_senTag(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/sentences\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['sentences']\n",
    "    \n",
    "    def get_relation(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/relationships\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['relationships']\n",
    "\n",
    "    def get_classification(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/categories\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0593829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from graphviz import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e93639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPbrl():\n",
    "    def __init__(self):\n",
    "        self.api_data = Data()\n",
    "\n",
    "    def __word_wash(self, text, stop_word='default'):\n",
    "        #determine language\n",
    "        if stop_word == 'default':\n",
    "            try:\n",
    "                lang = self.api_data.get_lang(text)\n",
    "            except:\n",
    "                print(\"Error with get language function\")\n",
    "            if lang == 'zho':\n",
    "                stop_word = 'pacakge_data/cn_stopwords.txt'\n",
    "            elif lang == 'eng':\n",
    "                stop_word = 'pacakge_data/en_stopwords.txt'\n",
    "            else:\n",
    "                stop_word = 'pacakge_data/other_stopwords.txt'\n",
    "\n",
    "        try:\n",
    "            stop_file = open(stop_word, 'r', encoding='utf-8')\n",
    "            stopwords = stop_file.read().split(\"\\n\")\n",
    "            stop_file.close()\n",
    "        except:\n",
    "            print('{} is not exist, please check the file!'.format(stop_word))\n",
    "\n",
    "        #check the characters limit\n",
    "        text_lst = self.__check_limit(text)\n",
    "        text_token = []\n",
    "        try:\n",
    "            for t in text_lst:\n",
    "                text_token.extend(self.api_data.get_token(t))\n",
    "        except:\n",
    "            print(\"Error with get token function\")\n",
    "\n",
    "        washed_token = []\n",
    "        for char in text_token:\n",
    "            if char in stopwords:\n",
    "                pass\n",
    "            else:\n",
    "                washed_token.append(char)\n",
    "        return washed_token\n",
    "\n",
    "    def cal_frequency(self, text, stop_word='default'):\n",
    "        washed_token = self.__word_wash(text, self.api_data, stop_word)\n",
    "\n",
    "        counts = {}\n",
    "        for word in washed_token:\n",
    "            counts[word] = counts.get(word, 0) + 1\n",
    "\n",
    "        counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return counts\n",
    "\n",
    "    def __check_limit(self, text):\n",
    "        text = text.replace('\\n', ' ')\n",
    "        if len(text.replace(' ', '')) <= 5000:\n",
    "            return [text]\n",
    "        else:\n",
    "            true_text = []\n",
    "            if '.' in text:\n",
    "                true_text = self.__check_limit_tool(text, '.')\n",
    "            elif '。' in text:\n",
    "                true_text = self.__check_limit_tool(text, '。')\n",
    "            else:\n",
    "                true_text = self.__check_limit_tool(text, '')\n",
    "        return true_text\n",
    "\n",
    "\n",
    "    def __check_limit_tool(text, symbol):\n",
    "        temp = []\n",
    "        string = ''\n",
    "        if symbol != '':\n",
    "            split_text = text.split(symbol)\n",
    "        else:\n",
    "            split_text = text\n",
    "        for sen in split_text:\n",
    "            temp_s = string + sen + symbol\n",
    "            if len(temp_s.replace(' ', '')) > 5000:\n",
    "                temp.append(string)\n",
    "                string = sen + symbol\n",
    "            else:\n",
    "                string = string + sen + symbol\n",
    "\n",
    "            if sen == split_text[-1]:\n",
    "                temp.append(string)\n",
    "        return temp\n",
    "\n",
    "    def word_viz(self, text, file_loc, top_num, stop_word='default', cloud_set=WordCloud(font_path='pacakge_data/STKAITI.TTF')):\n",
    "        washed_token = self.__word_wash(text, self.api_data, stop_word)\n",
    "    \n",
    "        cloud_set.generate(' '.join(washed_token))\n",
    "        cloud_set.to_file(file_loc + 'cloud.png')\n",
    "\n",
    "        counts = self.cal_frequency(text, self.api_data, stop_word)\n",
    "        words = []\n",
    "        count = []\n",
    "        for i in counts:\n",
    "            words.append(i[0])\n",
    "            count.append(i[1])\n",
    "        df = pd.DataFrame({'words': words, 'count': count})\n",
    "        chart = (alt.Chart(df[:top_num]).mark_line().encode(\n",
    "            x='words', y='count').properties(height=400, width=400))\n",
    "        save(chart, \"chart.html\")\n",
    "\n",
    "    #text: text which needs to be calculate tfidf\n",
    "    #document: document_loc to train idf\n",
    "    #topK, keywords for top x\n",
    "    def key_extra_tfidf(self, text, document, stop_word='default'):\n",
    "        text_wash_lst = []\n",
    "        for doc in document:\n",
    "            wash_token = self.__word_wash(doc, self.api_data, stop_word)\n",
    "            text_wash_lst.append(wash_token)\n",
    "        \n",
    "        text_wash = self.__word_wash(text, self.api_data, stop_word)\n",
    "\n",
    "        #word dictionary\n",
    "        set_lst = []\n",
    "        for i in text_wash_lst:\n",
    "            set_lst.extend(i)\n",
    "        wordSet = set(set_lst)\n",
    "\n",
    "        wordDict_lst = []\n",
    "        for wash_token in text_wash_lst:\n",
    "            temp_dic = dict.fromkeys(wordSet, 0)\n",
    "            for word in wash_token:\n",
    "                temp_dic[word] += 1\n",
    "            wordDict_lst.append(temp_dic)\n",
    "        \n",
    "        wordDict = dict.fromkeys(wordSet, 0)\n",
    "        for word in text_wash:\n",
    "            wordDict[word] += 1\n",
    "\n",
    "        tfDict = self.__cal_TF(wordDict, text_wash)\n",
    "        idfDict = self.__cal_IDF(wordDict_lst)\n",
    "\n",
    "        tfidf = self.__cal_TFIDF(tfDict, idfDict)\n",
    "\n",
    "        return tfidf\n",
    "\n",
    "    def __cal_TF(self, wordDict, wash_token):\n",
    "        tfDict = {}\n",
    "        token_count = len(wash_token)\n",
    "        for word, count in wordDict.items():\n",
    "            tfDict[word] = count / token_count\n",
    "        return tfDict\n",
    "\n",
    "    def __cal_IDF(self, wordDict_lst):\n",
    "        idfDict = dict.fromkeys(wordDict_lst[0], 0)\n",
    "        N = len(wordDict_lst)\n",
    "        for wordDict in wordDict_lst:\n",
    "            for word, count in wordDict.items():\n",
    "                if count > 0:\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "        for word, ni in idfDict.items():\n",
    "            idfDict[word] = math.log10(N / (ni + 1))\n",
    "\n",
    "        return idfDict\n",
    "\n",
    "    def __cal_TFIDF(self, tf, idf):\n",
    "        tfidf = {}\n",
    "        for word, tf_val in tf.items():\n",
    "            tfidf[word] = tf_val * idf[word]\n",
    "        return tfidf\n",
    "\n",
    "    def cal_simi(self, text1, text2, size='sen', method='euc'):\n",
    "        try:\n",
    "            token_1 = self.api_data.get_token(text1)\n",
    "            token_2 = self.api_data.get_token(text2)\n",
    "        except:\n",
    "            print(\"Error with get token function\") \n",
    "        \n",
    "        try:\n",
    "            if size == 'sen':\n",
    "                vec_1 = self.api_data.get_vec(text1)['documentEmbedding']\n",
    "                vec_2 = self.api_data.get_vec(text2)['documentEmbedding']\n",
    "            elif size == 'word':\n",
    "                vec_1 = self.api_data.get_vec(text1)['tokenEmbeddings']\n",
    "                vec_2 = self.api_data.get_vec(text2)['tokenEmbeddings']\n",
    "        except:\n",
    "            print(\"Error with get vector function\") \n",
    "            #padding\n",
    "            max_matrix_length = max(len(vec_1),len(vec_2))\n",
    "            add = np.zeros(300)\n",
    "            if max_matrix_length == len(vec_1):\n",
    "                vec_2.extend([list(add)]* (max_matrix_length-len(vec_2)))\n",
    "                vec_2 = np.array(vec_2).flatten()\n",
    "                vec_1 = np.array(vec_1).flatten()\n",
    "            else:\n",
    "                vec_1.extend([list(add)]* (max_matrix_length-len(vec_1)))\n",
    "                vec_1 = np.array(vec_1).flatten()\n",
    "                vec_2 = np.array(vec_2).flatten()\n",
    "        else:\n",
    "            print('There is no size '.format(size))\n",
    "\n",
    "        if method == 'euc':\n",
    "            score = self.__simi_cal_euc(vec_1, vec_2, size)\n",
    "        elif method == 'cos':\n",
    "            score = self.__simi_cal_cos(vec_1, vec_2, size)\n",
    "        elif method == 'jac':\n",
    "            score = self.__simi_cal_jac(token_1, token_2, size)\n",
    "        elif method == 'cheb':\n",
    "            score = self.__simi_cal_cheb(vec_1, vec_2, size)\n",
    "        elif method == 'mah':\n",
    "            score = self.__simi_cal_mah(vec_1, vec_2, size)\n",
    "        else:\n",
    "            print('There is no method '.format(method))\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def __simi_cal_euc(self, vec1,vec2,size):\n",
    "        if size=='word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.array(vec1)\n",
    "            vec2=np.array(vec2)\n",
    "        return float(np.sqrt(np.sum(np.square(vec1-vec2))))\n",
    "    \n",
    "    def __simi_cal_cos(self, vec1,vec2,size):\n",
    "        if size=='word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.array(vec1)\n",
    "            vec2=np.array(vec2)\n",
    "        return float(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "    \n",
    "    def __simi_cal_mah(self, vec1, vec2, size):\n",
    "        if size=='word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.array(vec1)\n",
    "            vec2=np.array(vec2)\n",
    "        return float(np.sum(np.abs(vec1-vec2)))\n",
    "\n",
    "    def __simi_cal_jac(self, token_1, token_2, size):\n",
    "        inter = len(list(set(token_1).intersection(token_2)))\n",
    "        union = (len(token_1) + len(token_1)) - inter\n",
    "        return float(inter) / union\n",
    "\n",
    "    def __simi_cal_cheb(self,vec1, vec2, size):\n",
    "        if size == 'word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.mat(vec1)\n",
    "            vec2=np.mat(vec2)\n",
    "        return float(np.max(np.abs(vec1-vec2)))\n",
    "\n",
    "    def cal_textRank(self, text, candidate_pos=['NOUN', 'PROPN', 'VERB'], top_k = 10, window_size=3, stop_word='default'):\n",
    "        #damping coefficient\n",
    "        damp = 0.85\n",
    "        #convergence threshold\n",
    "        min_conv = 1e-5\n",
    "        iters = 10\n",
    "        #keywords + weight\n",
    "        key_weight = {}\n",
    "    \n",
    "        try:\n",
    "            sentences = self.api_data.get_senTag(text)\n",
    "        except:\n",
    "            print(\"Error with get sentence tagging function\")\n",
    "    \n",
    "        try:\n",
    "            #filter pos tag\n",
    "            pos_tag_lst = []\n",
    "            for sen in sentences:\n",
    "                pos_tag_lst.append(self.api_data.get_posTag(sen))\n",
    "        except:\n",
    "            print(\"Error with get pos tagging function\")\n",
    "    \n",
    "        sen_filter_pos = []\n",
    "        for pos_dic in pos_tag_lst:\n",
    "            temp = []\n",
    "            tokens = pos_dic['tokens']\n",
    "            tags = pos_dic['posTags']\n",
    "            for i in range(0,len(tags)):\n",
    "                if tags[i] in candidate_pos:\n",
    "                    temp.append(tokens[i])\n",
    "            sen_filter_pos.append(temp)\n",
    "    \n",
    "\n",
    "        #determine language\n",
    "        if stop_word == 'default':\n",
    "            try:\n",
    "                lang = self.api_data.get_lang(text)\n",
    "            except:\n",
    "                print(\"Error with get language function\")\n",
    "            if lang == 'zho':\n",
    "                stop_word = 'pacakge_data/cn_stopwords.txt'\n",
    "            elif lang == 'eng':\n",
    "                stop_word = 'pacakge_data/en_stopwords.txt'\n",
    "            else:\n",
    "                stop_word = 'pacakge_data/other_stopwords.txt'\n",
    "\n",
    "        try:\n",
    "            stop_file = open(stop_word, 'r', encoding='utf-8')\n",
    "            stopwords = stop_file.read().split(\"\\n\")\n",
    "            stop_file.close()\n",
    "        except:\n",
    "            print('{} is not exist, please check the file!'.format(stop_word))\n",
    "\n",
    "        washed_token = []\n",
    "        for sen in sen_filter_pos:\n",
    "            temp = []\n",
    "            for char in sen:\n",
    "                if char in stopwords:\n",
    "                    pass\n",
    "                else:\n",
    "                    temp.append(char)\n",
    "            washed_token.append(temp)\n",
    "        \n",
    "        vocab_dic = OrderedDict()\n",
    "        count = 0\n",
    "        for sen in washed_token:\n",
    "            for token in sen:\n",
    "                if token not in vocab_dic:\n",
    "                    vocab_dic[token] = count\n",
    "                    count += 1\n",
    "                \n",
    "        token_pair_lst = []\n",
    "        for sen in washed_token:\n",
    "            for i,word in enumerate(sen):\n",
    "                for j in range(i+1,i+window_size):\n",
    "                    if j>=len(sen):\n",
    "                        break\n",
    "                    if (word,sen[j]) not in token_pair_lst:\n",
    "                        token_pair_lst.append((word,sen[j]))\n",
    "                    \n",
    "        #normalized matrix\n",
    "        matrix_size = len(vocab_dic)\n",
    "        matrix = np.zeros((matrix_size, matrix_size), dtype='float')\n",
    "        for word_pair in token_pair_lst:\n",
    "            word1 = vocab_dic[word_pair[0]]\n",
    "            word2 = vocab_dic[word_pair[1]]\n",
    "            matrix[word1][word2]=1\n",
    "        \n",
    "        #Symmeric matrix\n",
    "        matrix = matrix +matrix.T-np.diag(matrix.diagonal())\n",
    "    \n",
    "        #normalize\n",
    "        normalize = np.sum(matrix,axis=0)\n",
    "        matrix_norm = np.divide(matrix,normalize,where=normalize!=0)\n",
    "    \n",
    "        #inital weight\n",
    "        weight = np.array([1]*len(vocab_dic))\n",
    "        pre_weight = 0\n",
    "        for epoch in range(0,iters):\n",
    "            weight = (1-damp)+damp*np.dot(matrix_norm,weight)\n",
    "            if abs(pre_weight - sum(weight))  < min_conv:\n",
    "                break\n",
    "            else:\n",
    "                pre_weight=sum(weight)\n",
    "    \n",
    "        for word,index in vocab_dic.items():\n",
    "            key_weight[word]=weight[index]\n",
    "        \n",
    "        key_weight = OrderedDict(sorted(key_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        count = 0\n",
    "        keywords_lst = []\n",
    "        for key in key_weight:\n",
    "            keywords_lst.append((key,key_weight[key]))\n",
    "            count += 1\n",
    "            if count >= top_k:\n",
    "                break\n",
    "        return keywords_lst\n",
    "\n",
    "    def relation_viz(self, text):\n",
    "        text_lst = self.__check_limit(text)\n",
    "        relations = []\n",
    "        try:\n",
    "            for t in text_lst:\n",
    "                relations.extend(self.api_data.get_relation(t))\n",
    "        except:\n",
    "            print(\"Error with get relationship function\")\n",
    "\n",
    "        g = Graph('graph', filename='G', engine='neato')\n",
    "        for relation_dic in relations:\n",
    "            rela = relation_dic['predicate']\n",
    "            arg1 = relation_dic['arg1']\n",
    "            arg2 = relation_dic['arg2']\n",
    "            g.node(arg1)\n",
    "            g.node(arg2)\n",
    "            g.edge(arg1,arg2, label=rela,len='1.50')\n",
    "        \n",
    "        g.view()\n",
    "        return True\n",
    "\n",
    "    def cal_classification(self, text, top_k = 3):\n",
    "        try:\n",
    "            classify = self.api_data.get_classification(text)\n",
    "        except:\n",
    "            print(\"Error with get classification function\")\n",
    "            \n",
    "        if top_k > len(classify):\n",
    "            print('The k exceed the number of categories')\n",
    "            return False\n",
    "        else:\n",
    "            return classify[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779547b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
