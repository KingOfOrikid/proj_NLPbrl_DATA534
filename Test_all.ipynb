{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e8e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        #key1 \"ad9765417caa9e2fd304e0aea0e2d24f\"\n",
    "        #key2 \"4ccf98ac9854f73e845a16a9e300daeb\"\n",
    "        self.headers = {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"X-RosetteAPI-Key\": \"4ccf98ac9854f73e845a16a9e300daeb\",\n",
    "            \"X-RapidAPI-Key\":\n",
    "            \"bc44666f27msh0cea06e275c5146p185e39jsnb306b7888971\",\n",
    "            \"X-RapidAPI-Host\": \"rosetteapi-rosette-v1.p.rapidapi.com\"\n",
    "        }\n",
    "        self.payload = {}\n",
    "\n",
    "    def get_token(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://rosetteapi-rosette-v1.p.rapidapi.com/tokens\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['tokens']\n",
    "\n",
    "    def get_lang(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/language\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)[\"languageDetections\"][0]['language']\n",
    "\n",
    "    def get_vec(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/semantics/vector\"\n",
    "        self.payload['content'] = text\n",
    "        self.payload['options'] = {\"perToken\": True}\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return {\n",
    "            'documentEmbedding': eval(response.text)['documentEmbedding'],\n",
    "            'tokenEmbeddings': eval(response.text)['tokenEmbeddings']\n",
    "        }\n",
    "\n",
    "    def get_posTag(self, text):\n",
    "        self.payload = {}\n",
    "        null = ''\n",
    "        url = \"https://rosetteapi-rosette-v1.p.rapidapi.com/morphology/complete\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "\n",
    "        return {\n",
    "            \"tokens\": eval(response.text)[\"tokens\"],\n",
    "            \"posTags\": eval(response.text)[\"posTags\"]\n",
    "        }\n",
    "\n",
    "    def get_senTag(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/sentences\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['sentences']\n",
    "    \n",
    "    def get_relation(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/relationships\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)['relationships']\n",
    "\n",
    "    def get_classification(self, text):\n",
    "        self.payload = {}\n",
    "        url = \"https://api.rosette.com/rest/v1/categories\"\n",
    "        self.payload['content'] = text\n",
    "        response = requests.request(\"POST\",\n",
    "                                    url,\n",
    "                                    json=self.payload,\n",
    "                                    headers=self.headers)\n",
    "        return eval(response.text)[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0593829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from graphviz import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e93639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPbrl():\n",
    "    def __init__(self):\n",
    "        self.api_data = Data()\n",
    "\n",
    "    def __word_wash(self, text, stop_word='default'):\n",
    "        #determine language\n",
    "        if stop_word == 'default':\n",
    "            try:\n",
    "                lang = self.api_data.get_lang(text[:100])\n",
    "            except:\n",
    "                print(\"Error with get language function\")\n",
    "                return False\n",
    "            if lang == 'zho':\n",
    "                stop_word = 'pacakge_data/cn_stopwords.txt'\n",
    "            elif lang == 'eng':\n",
    "                stop_word = 'pacakge_data/en_stopwords.txt'\n",
    "            else:\n",
    "                stop_word = 'pacakge_data/other_stopwords.txt'\n",
    "        elif stop_word == 'None':\n",
    "            #check the characters limit\n",
    "            text_lst = self.__check_limit(text)\n",
    "            text_token = []\n",
    "            try:\n",
    "                for t in text_lst:\n",
    "                    text_token.extend(self.api_data.get_token(t))\n",
    "            except:\n",
    "                print(\"Error with get token function\")\n",
    "                return False\n",
    "\n",
    "            washed_token = text_token\n",
    "            return washed_token\n",
    "\n",
    "        try:\n",
    "            stop_file = open(stop_word, 'r', encoding='utf-8')\n",
    "            stopwords = stop_file.read().split(\"\\n\")\n",
    "            stop_file.close()\n",
    "        except:\n",
    "            print('{} is not exist, please check the file!'.format(stop_word))\n",
    "            return False\n",
    "\n",
    "        #check the characters limit\n",
    "        text_lst = self.__check_limit(text)\n",
    "        text_token = []\n",
    "        try:\n",
    "            for t in text_lst:\n",
    "                text_token.extend(self.api_data.get_token(t))\n",
    "        except:\n",
    "            print(\"Error with get token function\")\n",
    "            return False\n",
    "\n",
    "        washed_token = []\n",
    "        for char in text_token:\n",
    "            if char in stopwords:\n",
    "                pass\n",
    "            else:\n",
    "                washed_token.append(char)\n",
    "        return washed_token\n",
    "\n",
    "    def cal_frequency(self, text, stop_word='default'):\n",
    "        washed_token = self.__word_wash(text, stop_word)\n",
    "        if washed_token == False:\n",
    "            return False\n",
    "        counts = {}\n",
    "        for word in washed_token:\n",
    "            counts[word] = counts.get(word, 0) + 1\n",
    "\n",
    "        counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return counts\n",
    "\n",
    "    def __check_limit(self, text):\n",
    "        text = text.replace('\\n', ' ')\n",
    "        if len(text.replace(' ', '')) <= 5000:\n",
    "            return [text]\n",
    "        else:\n",
    "            true_text = []\n",
    "            if '.' in text:\n",
    "                true_text = self.__check_limit_tool(text, '.')\n",
    "            elif '。' in text:\n",
    "                true_text = self.__check_limit_tool(text, '。')\n",
    "            else:\n",
    "                true_text = self.__check_limit_tool(text, '')\n",
    "        return true_text\n",
    "\n",
    "\n",
    "    def __check_limit_tool(self, text, symbol):\n",
    "        temp = []\n",
    "        string = ''\n",
    "        if symbol != '':\n",
    "            split_text = text.split(symbol)\n",
    "        else:\n",
    "            split_text = text\n",
    "        for sen in split_text:\n",
    "            temp_s = string + sen + symbol\n",
    "            if len(temp_s.replace(' ', '')) > 5000:\n",
    "                temp.append(string)\n",
    "                string = sen + symbol\n",
    "            else:\n",
    "                string = string + sen + symbol\n",
    "\n",
    "            if sen == split_text[-1]:\n",
    "                temp.append(string)\n",
    "        return temp\n",
    "\n",
    "    def word_viz(self, text, file_loc, top_num, stop_word='default', cloud_set=WordCloud(font_path='pacakge_data/STKAITI.TTF')):\n",
    "        washed_token = self.__word_wash(text, stop_word)\n",
    "        \n",
    "        if washed_token == False:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            cloud_set.generate(' '.join(washed_token))\n",
    "        except:\n",
    "            print(\"Error with word cloud.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            cloud_set.to_file(file_loc + 'cloud.png')\n",
    "        except:\n",
    "            print(\"Error with saving word cloud.\")\n",
    "            return False\n",
    "\n",
    "        counts = self.cal_frequency(text, stop_word)\n",
    "        words = []\n",
    "        count = []\n",
    "        for i in counts:\n",
    "            words.append(i[0])\n",
    "            count.append(i[1])\n",
    "        df = pd.DataFrame({'words': words, 'count': count})\n",
    "        \n",
    "        if top_num >= len(df):\n",
    "            top_num = len(df) - 1\n",
    "            \n",
    "        chart = (alt.Chart(df[:top_num]).mark_line().encode(\n",
    "            x='words', y='count').properties(height=400, width=400))\n",
    "        try:\n",
    "            save(chart, \"chart.html\")\n",
    "        except:\n",
    "            print(\"Error with saving chart.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    #text: text which needs to be calculate tfidf\n",
    "    #document: document_loc to train idf\n",
    "    #topK, keywords for top x\n",
    "    def key_extra_tfidf(self, text, document, stop_word='default'):\n",
    "        if len(text)==0 or len(document)==0 or type(document)!=list:\n",
    "            print('Error with input type and content.')\n",
    "            return False\n",
    "        \n",
    "        text_wash_lst = []\n",
    "        for doc in document:\n",
    "            wash_token = self.__word_wash(doc, stop_word)\n",
    "            if wash_token == False:\n",
    "                return False\n",
    "            text_wash_lst.append(wash_token)\n",
    "        \n",
    "        text_wash = self.__word_wash(text, stop_word)\n",
    "        if text_wash == False:\n",
    "            return False\n",
    "\n",
    "        #word dictionary\n",
    "        set_lst = []\n",
    "        for i in text_wash_lst:\n",
    "            set_lst.extend(i)\n",
    "        wordSet = set(set_lst)\n",
    "\n",
    "        wordDict_lst = []\n",
    "        for wash_token in text_wash_lst:\n",
    "            temp_dic = dict.fromkeys(wordSet, 0)\n",
    "            for word in wash_token:\n",
    "                temp_dic[word] += 1\n",
    "            wordDict_lst.append(temp_dic)\n",
    "        \n",
    "        wordDict = dict.fromkeys(wordSet, 0)\n",
    "        for word in text_wash:\n",
    "            wordDict[word] += 1\n",
    "        \n",
    "        try:\n",
    "            tfDict = self.__cal_TF(wordDict, text_wash)\n",
    "            idfDict = self.__cal_IDF(wordDict_lst)\n",
    "\n",
    "            tfidf = self.__cal_TFIDF(tfDict, idfDict)\n",
    "        except:\n",
    "            print('Calculate error.')\n",
    "            return False\n",
    "        return tfidf\n",
    "\n",
    "    def __cal_TF(self, wordDict, wash_token):\n",
    "        tfDict = {}\n",
    "        token_count = len(wash_token)\n",
    "        for word, count in wordDict.items():\n",
    "            tfDict[word] = count / token_count\n",
    "        return tfDict\n",
    "\n",
    "    def __cal_IDF(self, wordDict_lst):\n",
    "        idfDict = dict.fromkeys(wordDict_lst[0], 0)\n",
    "        N = len(wordDict_lst)\n",
    "        for wordDict in wordDict_lst:\n",
    "            for word, count in wordDict.items():\n",
    "                if count > 0:\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "        for word, ni in idfDict.items():\n",
    "            idfDict[word] = math.log10((N+1) / (ni + 1))\n",
    "\n",
    "        return idfDict\n",
    "\n",
    "    def __cal_TFIDF(self, tf, idf):\n",
    "        tfidf = {}\n",
    "        for word, tf_val in tf.items():\n",
    "            tfidf[word] = tf_val * idf[word]\n",
    "        return tfidf\n",
    "\n",
    "    def cal_simi(self, text1, text2, size='sen', method='euc'):\n",
    "        try:\n",
    "            token_1 = self.api_data.get_token(text1)\n",
    "            token_2 = self.api_data.get_token(text2)\n",
    "        except:\n",
    "            print(\"Error with get token function\") \n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if size == 'sen':\n",
    "                vec_1 = self.api_data.get_vec(text1)['documentEmbedding']\n",
    "                vec_2 = self.api_data.get_vec(text2)['documentEmbedding']\n",
    "            elif size == 'word':\n",
    "                vec_1 = self.api_data.get_vec(text1)['tokenEmbeddings']\n",
    "                vec_2 = self.api_data.get_vec(text2)['tokenEmbeddings']\n",
    "                #padding\n",
    "                max_matrix_length = max(len(vec_1),len(vec_2))\n",
    "                add = np.zeros(300)\n",
    "                if max_matrix_length == len(vec_1):\n",
    "                    vec_2.extend([list(add)]* (max_matrix_length-len(vec_2)))\n",
    "                    vec_2 = np.array(vec_2).flatten()\n",
    "                    vec_1 = np.array(vec_1).flatten()\n",
    "                else:\n",
    "                    vec_1.extend([list(add)]* (max_matrix_length-len(vec_1)))\n",
    "                    vec_1 = np.array(vec_1).flatten()\n",
    "                    vec_2 = np.array(vec_2).flatten()\n",
    "            else:\n",
    "                print('There is no size '.format(size))\n",
    "                return False\n",
    "        except:\n",
    "            print(\"Error with get vector function\") \n",
    "            return False\n",
    "            \n",
    "\n",
    "\n",
    "        if method == 'euc':\n",
    "            score = self.__simi_cal_euc(vec_1, vec_2, size)\n",
    "        elif method == 'cos':\n",
    "            score = self.__simi_cal_cos(vec_1, vec_2, size)\n",
    "        elif method == 'jac':\n",
    "            score = self.__simi_cal_jac(token_1, token_2, size)\n",
    "        else:\n",
    "            print('There is no method '.format(method))\n",
    "            return False\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def __simi_cal_euc(self, vec1,vec2,size):\n",
    "        if size=='word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.array(vec1)\n",
    "            vec2=np.array(vec2)\n",
    "        return float(np.sqrt(np.sum(np.square(vec1-vec2))))\n",
    "    \n",
    "    def __simi_cal_cos(self, vec1,vec2,size):\n",
    "        if size=='word':\n",
    "            pass\n",
    "        else:\n",
    "            vec1=np.array(vec1)\n",
    "            vec2=np.array(vec2)\n",
    "        return float(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "\n",
    "    def __simi_cal_jac(self, token_1, token_2, size):\n",
    "        inter = len(list(set(token_1).intersection(token_2)))\n",
    "        union = (len(token_1) + len(token_1)) - inter\n",
    "        return float(inter) / union\n",
    "\n",
    "    def cal_textRank(self, text, candidate_pos=['NOUN', 'PROPN', 'VERB'], top_k = 10, window_size=3, stop_word='default'):\n",
    "        #damping coefficient\n",
    "        damp = 0.85\n",
    "        #convergence threshold\n",
    "        min_conv = 1e-5\n",
    "        iters = 10\n",
    "        #keywords + weight\n",
    "        key_weight = {}\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        if len(text.replace(' ', '')) > 5000:\n",
    "            print(\"Please control the length of input text\")\n",
    "            return False\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            sentences = self.api_data.get_senTag(text)\n",
    "        except:\n",
    "            print(\"Error with get sentence tagging function\")\n",
    "            return False\n",
    "    \n",
    "        try:\n",
    "            #filter pos tag\n",
    "            pos_tag_lst = []\n",
    "            for sen in sentences:\n",
    "                pos_tag_lst.append(self.api_data.get_posTag(sen))\n",
    "        except:\n",
    "            print(\"Error with get pos tagging function\")\n",
    "            return False\n",
    "    \n",
    "        sen_filter_pos = []\n",
    "        for pos_dic in pos_tag_lst:\n",
    "            temp = []\n",
    "            tokens = pos_dic['tokens']\n",
    "            tags = pos_dic['posTags']\n",
    "            for i in range(0,len(tags)):\n",
    "                if tags[i] in candidate_pos:\n",
    "                    temp.append(tokens[i])\n",
    "            sen_filter_pos.append(temp)\n",
    "    \n",
    "\n",
    "        #determine language\n",
    "        if stop_word == 'default':\n",
    "            try:\n",
    "                lang = self.api_data.get_lang(text)\n",
    "            except:\n",
    "                print(\"Error with get language function\")\n",
    "                return False\n",
    "            if lang == 'zho':\n",
    "                stop_word = 'pacakge_data/cn_stopwords.txt'\n",
    "            elif lang == 'eng':\n",
    "                stop_word = 'pacakge_data/en_stopwords.txt'\n",
    "            else:\n",
    "                stop_word = 'pacakge_data/other_stopwords.txt'\n",
    "        elif stop_word == 'None':\n",
    "            stop_word = 'pacakge_data/emp_stopwords.txt'\n",
    "\n",
    "        try:\n",
    "            stop_file = open(stop_word, 'r', encoding='utf-8')\n",
    "            stopwords = stop_file.read().split(\"\\n\")\n",
    "            stop_file.close()\n",
    "        except:\n",
    "            print('{} is not exist, please check the file!'.format(stop_word))\n",
    "            return False\n",
    "\n",
    "        washed_token = []\n",
    "        for sen in sen_filter_pos:\n",
    "            temp = []\n",
    "            for char in sen:\n",
    "                if char in stopwords:\n",
    "                    pass\n",
    "                else:\n",
    "                    temp.append(char)\n",
    "            washed_token.append(temp)\n",
    "        \n",
    "        vocab_dic = OrderedDict()\n",
    "        count = 0\n",
    "        for sen in washed_token:\n",
    "            for token in sen:\n",
    "                if token not in vocab_dic:\n",
    "                    vocab_dic[token] = count\n",
    "                    count += 1\n",
    "                \n",
    "        token_pair_lst = []\n",
    "        for sen in washed_token:\n",
    "            for i,word in enumerate(sen):\n",
    "                for j in range(i+1,i+window_size):\n",
    "                    if j>=len(sen):\n",
    "                        break\n",
    "                    if (word,sen[j]) not in token_pair_lst:\n",
    "                        token_pair_lst.append((word,sen[j]))\n",
    "                    \n",
    "        #normalized matrix\n",
    "        matrix_size = len(vocab_dic)\n",
    "        matrix = np.zeros((matrix_size, matrix_size), dtype='float')\n",
    "        for word_pair in token_pair_lst:\n",
    "            word1 = vocab_dic[word_pair[0]]\n",
    "            word2 = vocab_dic[word_pair[1]]\n",
    "            matrix[word1][word2]=1\n",
    "        \n",
    "        #Symmeric matrix\n",
    "        matrix = matrix +matrix.T-np.diag(matrix.diagonal())\n",
    "    \n",
    "        #normalize\n",
    "        normalize = np.sum(matrix,axis=0)\n",
    "        matrix_norm = np.divide(matrix,normalize,where=normalize!=0)\n",
    "    \n",
    "        #inital weight\n",
    "        weight = np.array([1]*len(vocab_dic))\n",
    "        pre_weight = 0\n",
    "        for epoch in range(0,iters):\n",
    "            weight = (1-damp)+damp*np.dot(matrix_norm,weight)\n",
    "            if abs(pre_weight - sum(weight))  < min_conv:\n",
    "                break\n",
    "            else:\n",
    "                pre_weight=sum(weight)\n",
    "    \n",
    "        for word,index in vocab_dic.items():\n",
    "            key_weight[word]=weight[index]\n",
    "        \n",
    "        key_weight = OrderedDict(sorted(key_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        count = 0\n",
    "        keywords_lst = []\n",
    "        for key in key_weight:\n",
    "            keywords_lst.append((key,key_weight[key]))\n",
    "            count += 1\n",
    "            if count >= top_k:\n",
    "                break\n",
    "        return keywords_lst\n",
    "\n",
    "    def relation_viz(self, text):\n",
    "        text_lst = self.__check_limit(text)\n",
    "        relations = []\n",
    "        try:\n",
    "            for t in text_lst:\n",
    "                relations.extend(self.api_data.get_relation(t))\n",
    "        except:\n",
    "            print(\"Error with get relationship function\")\n",
    "            return False\n",
    "\n",
    "        g = Graph('graph', filename='G', engine='neato')\n",
    "        for relation_dic in relations:\n",
    "            rela = relation_dic['predicate']\n",
    "            arg1 = relation_dic['arg1']\n",
    "            arg2 = relation_dic['arg2']\n",
    "            g.node(arg1)\n",
    "            g.node(arg2)\n",
    "            g.edge(arg1,arg2, label=rela,len='1.50')\n",
    "        \n",
    "        g.view()\n",
    "        return True\n",
    "\n",
    "    def cal_classification(self, text, top_k = 3):\n",
    "        try:\n",
    "            classify = self.api_data.get_classification(text)\n",
    "        except:\n",
    "            print(\"Error with get classification function\")\n",
    "            return False\n",
    "            \n",
    "        if top_k > len(classify):\n",
    "            print('The k exceed the number of categories')\n",
    "            return False\n",
    "        else:\n",
    "            return classify[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6576c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=NLPbrl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e318750",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"simple test of calculate frequency.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25168137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simple', 1), ('test', 1), ('calculate', 1), ('frequency', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_frequency(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a5582a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.word_viz(text, '', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3b216283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 1.0), ('calculate', 1.0), ('frequency', 1.0)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_textRank('simple test of calculate frequency.', candidate_pos=['NOUN', 'PROPN', 'VERB'], top_k = 10, window_size=3, stop_word='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "30b3bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The goal of the package is to wrap into a set of R/Python functions a web REST API and offer a package for others to use those functions. The functions offered by the package should also take care of the minimum wrangling necessary to output the data in a viable format (that is, not as a raw binary file, unless a raw binary file is the most appropriate data format). You choose the API and the most appropriate input / output design of your functions. Think carefully of what part of the wrangling is of general interest (i.e., most or all of the users will want to perform it, and thus should be done in the package) and what part is only relevant as an example for the vignette (i.e., is too specific to be of general interest, and thus may just end up in the vignette.)\"*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bfa88885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_frequency(text)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a255a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with saving word cloud.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.word_viz(text, 'test123/', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ec9786ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please control the length of input text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_textRank(text, candidate_pos=['NOUN', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "96fcdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"这是一个简单的代码测试代码块，用来测试中文的词频计算\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f098cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('代码', 2),\n",
       " ('测试', 2),\n",
       " ('一个', 1),\n",
       " ('简单', 1),\n",
       " ('块', 1),\n",
       " ('中文', 1),\n",
       " ('词频', 1),\n",
       " ('计算', 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_frequency(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c93ecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.word_viz(text, '', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dfff6863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('测试', 1.586140773809524),\n",
       " ('代码', 1.152276488095238),\n",
       " ('中文', 1.0696949404761904),\n",
       " ('块', 1.053261607142857),\n",
       " ('词频', 0.9007574404761904),\n",
       " ('计算', 0.6608854166666667),\n",
       " ('一个', 0.5769833333333333)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_textRank(text, candidate_pos=['NOUN', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "74241ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb046548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get language function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_frequency(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3f99efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get language function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.word_viz(text, '', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1dba4cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get sentence tagging function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_textRank(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bb9448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "    \"let's begin our test.\",\n",
    "]\n",
    "text = 'This is the test.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00b22f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23856062735983122"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.key_extra_tfidf(text, corpus)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9b524fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on my bed\",\n",
    "    \"The dog sat on my knees\"\n",
    "]\n",
    "text = \"The dog sat on my knees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "165cf022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029348543175946873"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.key_extra_tfidf(text, corpus,'None')['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef192176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with input type and content.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.key_extra_tfidf('', corpus,'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "140b5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with input type and content.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.key_extra_tfidf('', '','None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "da46e306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40277734306421165"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'sen', 'euc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1c91624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4605218363346508"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'word', 'euc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a59daa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188851939644076"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'sen', 'cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64fa31c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7866876207707693"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'word', 'cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e0943b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple',method = 'jac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7b58a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get token function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('', 'She is eating an apple', 'sen', 'euc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4aaae84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no size \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e8e4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no method \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_simi('I am eating an apple', 'She is eating an apple', 'sen','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a00c4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rela_text = \"FLIR Systems is headquartered in Oregon and produces thermal imaging, night vision, and infrared cameras and sensor systems.  According to the SEC’s order instituting a settled administrative proceeding, FLIR entered into a multi-million dollar contract to provide thermal binoculars to the Saudi government in November 2008.  Timms and Ramahi were the primary sales employees responsible for the contract, and also were involved in negotiations to sell FLIR’s security cameras to the same government officials.  At the time, Timms was the head of FLIR’s Middle East office in Dubai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6c4e0b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.relation_viz(rela_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5f73a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get relationship function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.relation_viz('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3fae4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_text = \"Sony Pictures is planning to shoot a good portion of the new \\\"Ghostbusters\\\" in Boston as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9cf9fad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ARTS_AND_ENTERTAINMENT',\n",
       "  'confidence': 0.06416648,\n",
       "  'score': -0.01447566},\n",
       " {'label': 'SPORTS', 'confidence': 0.05782175, 'score': -0.11859164},\n",
       " {'label': 'TRAVEL', 'confidence': 0.05627946, 'score': -0.14562697}]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_classification(cate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cf5a0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k exceed the number of categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_classification(cate_text,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9beb39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get classification function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.cal_classification('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4c0fd",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bce0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "##########import NLPbrl\n",
    "\n",
    "\n",
    "class TestProj(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        self.nlp = NLPbrl()\n",
    "        self.short_text_en = \"simple test of calculate frequency.\"\n",
    "        self.freq_short_res_en = [('simple', 1), ('test', 1), ('calculate', 1), ('frequency', 1)]\n",
    "        self.long_text_en = \"The goal of the package is to wrap into a set of R/Python functions a web REST API and offer a package for others to use those functions. The functions offered by the package should also take care of the minimum wrangling necessary to output the data in a viable format (that is, not as a raw binary file, unless a raw binary file is the most appropriate data format). You choose the API and the most appropriate input / output design of your functions. Think carefully of what part of the wrangling is of general interest (i.e., most or all of the users will want to perform it, and thus should be done in the package) and what part is only relevant as an example for the vignette (i.e., is too specific to be of general interest, and thus may just end up in the vignette.)\"*20\n",
    "        self.short_text_cn = \"这是一个简单的代码测试代码块，用来测试中文的词频计算\"\n",
    "        self.freq_short_res_cn = [('代码', 2), ('测试', 2), ('一个', 1), ('简单', 1), ('块', 1), ('中文', 1), ('词频', 1), ('计算', 1)]\n",
    "        self.empty_text = \"\"\n",
    "        self.tf_idf_corpus1 = ['This is the first document.',\n",
    "                        'This document is the second document.',\n",
    "                        'And this is the third one.',\n",
    "                        'Is this the first document?',\n",
    "                        \"let's begin our test.\",]\n",
    "        self.tf_idf_text1 = 'This is the test.'\n",
    "        self.tf_idf_corpus2 = [\"The cat sat on my bed\",\n",
    "                                \"The dog sat on my knees\"]\n",
    "        self.tf_idf_text2 = \"The dog sat on my knees\"\n",
    "        self.simi_text1 = 'I am eating an apple'\n",
    "        self.simi_text2 = 'She is eating an apple'\n",
    "        self.rank_short_res_en = [('test', 1.0), ('calculate', 1.0), ('frequency', 1.0)]\n",
    "        self.rela_text = \"FLIR Systems is headquartered in Oregon and produces thermal imaging, night vision, and infrared cameras and sensor systems.  According to the SEC’s order instituting a settled administrative proceeding, FLIR entered into a multi-million dollar contract to provide thermal binoculars to the Saudi government in November 2008.  Timms and Ramahi were the primary sales employees responsible for the contract, and also were involved in negotiations to sell FLIR’s security cameras to the same government officials.  At the time, Timms was the head of FLIR’s Middle East office in Dubai.\"\n",
    "        self.cate_text = \"Sony Pictures is planning to shoot a good portion of the new \\\"Ghostbusters\\\" in Boston as well.\"\n",
    "        self.cate_res = [{'label': 'ARTS_AND_ENTERTAINMENT','confidence': 0.06416648,'score': -0.01447566},\n",
    "                        {'label': 'SPORTS', 'confidence': 0.05782175, 'score': -0.11859164},\n",
    "                        {'label': 'TRAVEL', 'confidence': 0.05627946, 'score': -0.14562697}]\n",
    "        \n",
    "    @classmethod\n",
    "    def tearDownClass(self):\n",
    "        print('successful')\n",
    "        \n",
    "    def test_cal_frequency(self):\n",
    "        self.assertEqual(self.nlp.cal_frequency(self.short_text_en),self.freq_short_res_en)\n",
    "        self.assertEqual(self.nlp.cal_frequency(self.long_text_en)[0][1],self.long_text_en.count('package'))\n",
    "        self.assertEqual(self.nlp.cal_frequency(self.short_text_cn),self.freq_short_res_cn)\n",
    "        self.assertEqual(self.nlp.cal_frequency(self.empty_text),False)\n",
    "\n",
    "    def test_word_viz(self):\n",
    "        self.assertEqual(self.nlp.word_viz(self.short_text_en, '', 10),True)\n",
    "        self.assertEqual(self.nlp.word_viz(self.long_text_en, 'test123/', 10),False)\n",
    "        self.assertEqual(self.nlp.word_viz(self.short_text_cn, '', 10),True)\n",
    "        self.assertEqual(self.nlp.word_viz(self.empty_text, '', 10),False)\n",
    "\n",
    "    def test_extra_tfidf(self):\n",
    "        self.assertAlmostEqual(self.nlp.key_extra_tfidf(self.tf_idf_text1, self.tf_idf_corpus1)['test'],0.2386,2)\n",
    "        self.assertAlmostEqual(self.nlp.key_extra_tfidf(self.tf_idf_text2, self.tf_idf_corpus2, 'None')['dog'],0.029,2)\n",
    "        self.assertEqual(self.nlp.key_extra_tfidf(self.empty_text, self.tf_idf_corpus2),False)\n",
    "        self.assertEqual(self.nlp.key_extra_tfidf(self.empty_text, self.empty_text),False)\n",
    "\n",
    "    def test_cal_simi(self):\n",
    "        self.assertAlmostEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'sen','euc'),0.40277734306421165)\n",
    "        self.assertAlmostEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'word','euc'),1.46,2)\n",
    "        self.assertAlmostEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'sen','cos'),0.92,2)\n",
    "        self.assertAlmostEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'word','cos'),0.787,2)\n",
    "        self.assertAlmostEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,method='jac'),0.429,2)\n",
    "        self.assertEqual(self.nlp.cal_simi(self.empty_text,self.simi_text2,'sen','euc'),False)\n",
    "        self.assertEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'a'),False)\n",
    "        self.assertEqual(self.nlp.cal_simi(self.simi_text1,self.simi_text2,'sen','a'),False)\n",
    "        \n",
    "    def test_textRank(self):\n",
    "        self.assertEqual(self.nlp.cal_textRank(self.short_text_en),self.rank_short_res_en)\n",
    "        self.assertAlmostEqual(self.nlp.cal_textRank(self.long_text_en, candidate_pos=['NOUN', 'VERB']),False)\n",
    "        self.assertAlmostEqual(self.nlp.cal_textRank(self.short_text_cn, candidate_pos=['NOUN', 'VERB'])[0][1],1.586,2)\n",
    "        self.assertEqual(self.nlp.cal_textRank(self.empty_text),False)\n",
    "\n",
    "    def test_relation_viz(self):\n",
    "        self.assertEqual(self.nlp.relation_viz(self.rela_text),True)\n",
    "        self.assertEqual(self.nlp.relation_viz(self.empty_text),False)\n",
    "\n",
    "    def test_cal_classification(self):\n",
    "        self.assertEqual(self.nlp.cal_classification(self.cate_text),self.cate_res)\n",
    "        self.assertEqual(self.nlp.cal_classification(self.cate_text, 100),False)\n",
    "        self.assertEqual(self.nlp.cal_classification(self.empty_text),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b78c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k exceed the number of categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get classification function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get language function\n",
      "Error with get token function\n",
      "There is no size \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no method \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with input type and content.\n",
      "Error with input type and content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get relationship function\n",
      "Please control the length of input text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get sentence tagging function\n",
      "Error with saving word cloud.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get language function\n",
      "successful\n",
      "<unittest.runner.TextTestResult run=7 errors=0 failures=0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 31.279s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "test_cases = (TestProj)\n",
    "\n",
    "def test_suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    result = unittest.TestResult()\n",
    "    suite.addTest(unittest.makeSuite(TestProj))\n",
    "    runner = unittest.TextTestRunner()\n",
    "    print(runner.run(suite))\n",
    "test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8c611897",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=TestProj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fa0ce804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with get language function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.nlp.key_extra_tfidf(a.tf_idf_text1, a.tf_idf_corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "da6a73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the test.'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tf_idf_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7a05073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document.',\n",
       " 'This document is the second document.',\n",
       " 'And this is the third one.',\n",
       " 'Is this the first document?',\n",
       " \"let's begin our test.\"]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tf_idf_corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ed86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
